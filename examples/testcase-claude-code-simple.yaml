# Example: Basic Claude Code Test Case
#
# Demonstrates basic Claude Code agent configuration with inline prompt.
# Use this as a template for simple evaluation scenarios.

name: "Basic Claude Code Evaluation"
description: "Run Claude Code to analyze a repository and generate evaluation results"

# Repository configuration
repo: https://github.com/youbencha/hello-world.git
branch: main

# Agent configuration
agent:
  type: claude-code
  config:
    prompt: "Add a comment to README explaining what this repository is about"

# Evaluators to run
evaluators:
  - name: git-diff
    config: {}

  # agentic-judge: Uses AI to evaluate quality based on assertions
  - name: agentic-judge
    config:
      type: claude-code
      agent_name: agentic-judge  # Uses the built-in evaluation agent
      # Define explicit pass/fail assertions (keys become metric names in the report)
      assertions:
        readme_was_modified: "The README.md file was modified. Score 1 if true, 0 if false."
        message_is_friendly: "A friendly welcome message was added. Score 1 if friendly, 0.5 if neutral, 0 if absent or unfriendly."
        no_errors: "No syntax errors or broken markdown. Score 1 if valid, 0 if broken."

# Optional: Timeout in milliseconds (default: 5 minutes)
timeout: 300000
